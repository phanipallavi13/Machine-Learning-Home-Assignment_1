# Homework 1 â€” Linear Regression: Closed-form vs Gradient Descent

## ğŸ‘¤ Student Info
- **Name:** Phani Pallavi Tallavajjhala  
- **Student ID:** 700766070 

---

## ğŸ“Œ Overview
This assignment covers both theory and programming tasks for Linear Regression, including function approximation, random guessing, gradient descent, and recognizing underfitting/overfitting. It also compares the **Closed-form (Normal Equation)** and **Gradient Descent** methods on synthetic data.

---

## ğŸ—‚ Repository Contents
- `Homework1_LinearRegression.ipynb` â€” Jupyter Notebook with dataset generation, closed-form, and GD implementation.
- `synthetic_data.csv` â€” synthetic dataset (200 samples, x âˆˆ [0,5], y = 3 + 4x + Îµ).
- `ML ASSIGNMENT_1.txt` â€” written answers for Questions 1â€“6 (theory tasks).
- `linear_regression_fits.png` â€” plot showing raw data points + both fitted lines.
- `gd_loss_curve.png` â€” plot showing GD loss vs iterations.

---

## â–¶ï¸ How to Run

### Option A â€” Jupyter Notebook
1. Open `Homework1_LinearRegression.ipynb` in Jupyter.
2. Run all cells in order to generate data, fit both models, and display plots.

### Option B â€” Python Script
If you have a script version (`hw1_linear_regression.py`):
```bash
python hw1_linear_regression.py
```
This will print estimated parameters and save plots.

---

## ğŸ“Š Results (Sample Run)
- **Closed-form solution:** intercept â‰ˆ 2.69, slope â‰ˆ 4.13  
- **Gradient Descent (Î·=0.05, 1000 iters):** intercept â‰ˆ 2.69, slope â‰ˆ 4.13  
- **Loss curve:** decreases smoothly and stabilizes by ~1000 iterations  

The fitted lines overlap visually, confirming that GD converged to the same solution as the Normal Equation.

---

## ğŸ“ Short Explanation
The Normal Equation computes the exact least-squares solution directly. Gradient Descent starts from [0,0] and reduces error iteratively using the slope of the cost function. With Î·=0.05 for 1000 iterations, GD converges to nearly the same intercept and slope as the closed-form method, as shown by the overlapping lines and decreasing loss.

---

## ğŸ’¡ Notes
- All code is **commented** for clarity.  
- Theory answers are included in `ML ASSIGNMENT_1.txt`.  
